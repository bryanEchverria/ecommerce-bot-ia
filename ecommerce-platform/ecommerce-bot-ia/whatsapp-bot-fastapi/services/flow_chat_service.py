"""
Chat service integrado con Flow para pagos
Multi-tenant compatible
"""
import json
import os
from sqlalchemy.orm import Session
from models import FlowProduct, FlowPedido, FlowProductoPedido, FlowSesion
from services.flow_service import crear_orden_flow, get_client_id_for_phone
from services.backoffice_integration import (
    get_real_products_from_backoffice, 
    update_product_stock,
    get_product_by_name_fuzzy,
    get_tenant_from_phone,
    get_tenant_info,
    format_price
)
from datetime import datetime

# Smart flows integration (DESHABILITADO - USAR MOTOR GPT REASONING)
try:
    from services.smart_flows import detectar_intencion_con_gpt, ejecutar_flujo_inteligente
    SMART_FLOWS_AVAILABLE = False  # Forzar deshabilitado para usar motor GPT reasoning
except ImportError:
    SMART_FLOWS_AVAILABLE = False

# AI Improvements integration (SIEMPRE ACTIVO)
try:
    from services.ai_improvements import handle_message_with_context
    AI_IMPROVEMENTS_AVAILABLE = True
except ImportError:
    AI_IMPROVEMENTS_AVAILABLE = False

# OpenAI integration (if available)
try:
    import openai
    openai.api_key = os.getenv("OPENAI_API_KEY")
    OPENAI_AVAILABLE = bool(os.getenv("OPENAI_API_KEY"))
except ImportError:
    OPENAI_AVAILABLE = False

# Multi-tenant client mapping with Flow support
TENANT_CLIENTS = {
    "+3456789012": {
        "name": "Green House",
        "type": "cannabis",
        "client_id": "green_house",
        "greeting": "üåø ¬°Hola! Bienvenido a Green House\nEspecialistas en productos can√°bicos premium."
    },
    "+1234567890": {
        "name": "Demo Company", 
        "type": "electronics",
        "client_id": "demo_company",
        "greeting": "üì± ¬°Hola! Demo Company - Electr√≥nicos de calidad"
    },
    "+5678901234": {
        "name": "Mundo Canino",
        "type": "pets",
        "client_id": "mundo_canino", 
        "greeting": "üêï ¬°Hola! Mundo Canino - Todo para tu mascota"
    },
    "+9876543210": {
        "name": "Test Store",
        "type": "clothing",
        "client_id": "test_store",
        "greeting": "üëï ¬°Hola! Test Store - Moda y estilo"
    },
}

def menu_principal(client_info, productos):
    """Muestra el cat√°logo de productos directamente en lugar de un men√∫ gen√©rico"""
    catalogo = f"üåø *{client_info['name']} - Cat√°logo disponible:*\n\n"
    for i, prod in enumerate(productos, 1):
        stock_status = "‚úÖ Disponible" if prod['stock'] > 5 else f"‚ö†Ô∏è Quedan {prod['stock']}"
        catalogo += f"{i}. **{prod['name']}** - ${prod['price']:,.0f}\n"
        catalogo += f"   {prod['description']}\n"
        catalogo += f"   {stock_status}\n\n"
    catalogo += "üí¨ *Para comprar:* Escribe el nombre del producto que quieres\n"
    catalogo += "üìù *Ejemplo:* 'Quiero Northern Lights' o solo 'Northern Lights'"
    return catalogo

def procesar_con_openai_contextual(tenant_id: str, tenant_info: dict, mensaje: str, productos: list, historial: list) -> str:
    """
    Sistema 100% din√°mico usando OpenAI con contexto conversacional
    Escalable para cualquier tenant y tipo de negocio
    """
    import os
    from openai import OpenAI
    
    # Preparar datos din√°micos del tenant
    productos_contexto = ""
    if productos:
        productos_contexto = "\n".join([
            f"- {p['name']}: ${p['price']:,} (Stock: {p['stock']}) - {p['description']}"
            for p in productos  # DIN√ÅMICO: incluye TODOS los productos
        ])
    
    # Preparar historial conversacional
    historial_contexto = ""
    if historial:
        print(f"üîç Procesando historial: {historial}")
        for msg in historial[-5:]:
            for role, content in msg.items():
                if content:
                    historial_contexto += f"{role.upper()}: {content}\n"
        print(f"üîç Historial formateado: {repr(historial_contexto)}")
    
    # Prompt din√°mico y escalable
    system_prompt = f"""Eres el asistente de ventas de {tenant_info['name']}, especializado en {tenant_info['type']}.

DATOS REALES DEL INVENTARIO:
{productos_contexto}

CONVERSACI√ìN PREVIA:
{historial_contexto}

INSTRUCCIONES:
1. SIEMPRE usa los precios y stock EXACTOS del inventario
2. Mant√©n coherencia con la conversaci√≥n previa
3. Responde espec√≠ficamente sobre productos reales disponibles
4. Si el cliente ya pregunt√≥ algo, no repitas informaci√≥n b√°sica
5. S√© conversacional y recuerda el contexto
6. M√°ximo 300 caracteres para WhatsApp

MENSAJE ACTUAL: "{mensaje}"

Responde como el asistente experto de {tenant_info['name']}:"""

    print(f"üîç System prompt: {system_prompt[:200]}...")
    
    try:
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": mensaje}
            ],
            max_tokens=300,
            temperature=0.7
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        print(f"üö® Error OpenAI: {e}")
        # Fallback din√°mico simple
        return generar_respuesta_fallback_dinamica(tenant_info, mensaje, productos, historial)

def generar_respuesta_fallback_dinamica(tenant_info: dict, mensaje: str, productos: list, historial: list) -> str:
    """
    Fallback inteligente y din√°mico cuando OpenAI falla
    Sin hardcodear categor√≠as - usa solo datos reales
    """
    mensaje_lower = mensaje.lower()
    
    # Analizar si hay contexto previo
    contexto_previo = any(msg for msg in historial if any(content for content in msg.values()))
    
    # Saludo inteligente basado en contexto
    if any(word in mensaje_lower for word in ["hola", "hello", "hi", "buenas"]):
        if not contexto_previo:
            productos_resumen = f"{len(productos)} productos disponibles" if productos else "productos exclusivos"
            return f"¬°Hola! Bienvenido a {tenant_info['name']} üòä Tenemos {productos_resumen}. ¬øEn qu√© puedo ayudarte?"
        else:
            return f"¬°Hola de nuevo! ¬øEn qu√© m√°s puedo ayudarte en {tenant_info['name']}?"
    
    # B√∫squeda din√°mica en productos reales
    productos_mencionados = []
    for producto in productos:
        if any(word in producto['name'].lower() for word in mensaje_lower.split() if len(word) > 2):
            productos_mencionados.append(producto)
    
    if productos_mencionados:
        producto = productos_mencionados[0]
        return f"‚úÖ {producto['name']}: ${producto['price']:,} - Stock: {producto['stock']} unidades. {producto['description'][:50]}... ¬øTe interesa?"
    
    # Consulta de precios din√°mica
    if any(word in mensaje_lower for word in ["precio", "cuesta", "cu√°nto"]):
        if productos:
            producto = productos[0]
            return f"Te puedo ayudar con precios. Por ejemplo: {producto['name']} cuesta ${producto['price']:,}. ¬øQu√© producto espec√≠fico te interesa?"
    
    # Respuesta gen√©rica din√°mica
    categorias_detectadas = set()
    for producto in productos:
        # Extraer categor√≠as din√°micamente de nombres de productos
        words = producto['name'].lower().split()
        for word in words:
            if len(word) > 4:  # Palabras significativas
                categorias_detectadas.add(word)
    
    if categorias_detectadas:
        cats_str = ", ".join(list(categorias_detectadas)[:3])
        return f"En {tenant_info['name']} tenemos: {cats_str} y m√°s. ¬øQu√© te interesa espec√≠ficamente?"
    
    return f"¬°Perfecto! En {tenant_info['name']} puedo ayudarte con nuestros productos. ¬øHay algo espec√≠fico que buscas?"

def procesar_mensaje_flow_inteligente(db: Session, telefono: str, mensaje: str, tenant_id: str = None, historial: list = None) -> str:
    """
    Procesamiento inteligente multi-tenant que SIEMPRE usa IA para tomar decisiones
    La IA decide autom√°ticamente el flujo apropiado basado en contexto y historial
    """
    print(f"üß† PROCESAMIENTO INTELIGENTE: '{mensaje}' | Tenant: {tenant_id} | Historial: {len(historial) if historial else 0}")
    
    # Obtener informaci√≥n din√°mica del tenant
    if tenant_id is None:
        tenant_id = get_tenant_from_phone(telefono, db)
    
    tenant_info = get_tenant_info(tenant_id, db)
    productos = get_real_products_from_backoffice(db, tenant_id)
    
    print(f"üîç PRODUCTOS CONSULTADOS: {len(productos)} productos encontrados")
    for i, p in enumerate(productos):
        if i < 10 or p.get('category', '').lower() == 'semillas':
            print(f"   {i+1}. {p.get('name', 'Sin nombre')}: ${p.get('price', 0)} | Stock: {p.get('stock', 0)} | Categor√≠a: {p.get('category', 'Sin categor√≠a')}")
    
    # Usar historial tal como viene (ya est√° en formato correcto)
    ai_history = historial[-5:] if historial else []
    
    # Sistema din√°mico de IA con contexto
    print(f"üöÄ Procesando con contexto: {len(ai_history)} mensajes previos")
    
    try:
        # Usar OpenAI directamente de forma robusta
        response = procesar_con_openai_contextual(
            tenant_id=tenant_id,
            tenant_info=tenant_info,
            mensaje=mensaje,
            productos=productos,
            historial=ai_history
        )
        
        print(f"‚úÖ IA contextual exitosa: {response[:50]}...")
        return response
        
    except Exception as e:
        print(f"‚ùå Error en IA contextual: {e}")
        # Fallback al procesamiento normal
        return procesar_mensaje_flow(db, telefono, mensaje, tenant_id)

def obtener_sesion(db: Session, telefono: str, tenant_id: str):
    """Obtiene o crea una sesi√≥n para el usuario"""
    sesion = db.query(FlowSesion).filter_by(telefono=telefono).first()
    if not sesion:
        sesion = FlowSesion(
            telefono=telefono,
            tenant_id=tenant_id,
            estado="INITIAL",
            datos="{}"
        )
        db.add(sesion)
        db.commit()
        db.refresh(sesion)
    return sesion

def guardar_sesion(db: Session, sesion, estado: str = None, datos: dict = None):
    """Guarda cambios en la sesi√≥n"""
    if estado:
        sesion.estado = estado
    if datos:
        sesion.datos = json.dumps(datos)
    sesion.updated_at = datetime.utcnow()
    db.commit()

def obtener_productos_cliente_real(db: Session, telefono: str, tenant_id: str = None):
    """
    Obtiene productos reales del backoffice en tiempo real
    COMPLETAMENTE DIN√ÅMICO - Multi-tenant compatible
    Puede usar n√∫mero de tel√©fono o tenant_id directamente
    """
    # Si no se proporciona tenant_id, obtenerlo del tel√©fono
    if tenant_id is None:
        tenant_id = get_tenant_from_phone(telefono, db)
    
    productos = get_real_products_from_backoffice(db, tenant_id)
    tenant_info = get_tenant_info(tenant_id, db)
    
    return productos, tenant_id, tenant_info

def procesar_mensaje_flow(db: Session, telefono: str, mensaje: str, tenant_id: str = None) -> str:
    """
    Procesa mensajes con l√≥gica de Flow integrada
    Multi-tenant compatible - consulta productos reales del backoffice
    """
    print(f"üöÄüöÄüöÄ ENTRANDO A PROCESAR_MENSAJE_FLOW: '{mensaje}' para tenant: {tenant_id}")
    # Si se proporciona tenant_id directamente (webhooks din√°micos), usarlo
    # Si no, obtener informaci√≥n del tenant basado en el tel√©fono (DIN√ÅMICO)
    if tenant_id is None:
        tenant_id = get_tenant_from_phone(telefono, db)
    
    tenant_info = get_tenant_info(tenant_id, db)
    
    # Mantener compatibilidad con c√≥digo existente
    client_info = {
        "name": tenant_info["name"],
        "type": tenant_info["type"],
        "greeting": tenant_info["greeting"]
    }
    
    if not client_info:
        return f"""‚ùå Cliente no configurado: {telefono}
        
‚úÖ Clientes v√°lidos:
‚Ä¢ +3456789012 ‚Üí Green House (can√°bicos)
‚Ä¢ +1234567890 ‚Üí Demo Company (electr√≥nicos)  
‚Ä¢ +5678901234 ‚Üí Mundo Canino (mascotas)
‚Ä¢ +9876543210 ‚Üí Test Store (ropa)

üß™ PRUEBA: Usa uno de estos n√∫meros"""

    sesion = obtener_sesion(db, telefono, tenant_id)
    datos_sesion = json.loads(sesion.datos) if sesion.datos else {}
    
    mensaje_lower = mensaje.lower().strip()
    
    # ========================================
    # PRIORIDAD ABSOLUTA: CONFIRMACI√ìN DE PEDIDOS
    # ========================================
    if sesion.estado == "ORDER_CONFIRMATION":
        print(f"‚ö†Ô∏è Estado ORDER_CONFIRMATION detectado, mensaje: '{mensaje}'")
        if any(word in mensaje_lower for word in ["s√≠", "si", "yes", "confirmo", "ok", "acepto"]):
            print(f"‚úÖ Confirmaci√≥n detectada!")
            datos = json.loads(sesion.datos)
            pedido_data = datos["pedido"]
            total = datos["total"]
            
            # Obtener datos del tenant para el pedido
            productos, tenant_id, tenant_info = obtener_productos_cliente_real(db, telefono, tenant_id)
            
            # Crear pedido en BD
            pedido = FlowPedido(
                telefono=telefono,
                tenant_id=tenant_id,
                total=total,
                estado="pendiente_pago"
            )
            db.add(pedido)
            db.commit()
            db.refresh(pedido)
            
            # Crear productos del pedido y actualizar stock en tiempo real
            for prod_id, item in pedido_data.items():
                # Actualizar stock en la tabla products del backoffice
                stock_actualizado = update_product_stock(db, prod_id, item["cantidad"], tenant_id)
                if not stock_actualizado:
                    return f"‚ùå Error: No hay suficiente stock de {item['nombre']}. Intenta con menos cantidad."
                
                producto_pedido = FlowProductoPedido(
                    pedido_id=pedido.id,
                    producto_id=prod_id,  # Usar string ID del backoffice
                    cantidad=item["cantidad"],
                    precio_unitario=item["precio"]
                )
                db.add(producto_pedido)
            
            db.commit()
            
            # Crear orden de pago en Flow
            descripcion = f"Pedido_{client_info['name']}_{pedido.id}"
            url_pago = crear_orden_flow(str(pedido.id), int(total), descripcion, tenant_id, db)
            
            # Preparar resumen del pedido con formato mejorado
            resumen_productos = ""
            for item in pedido_data.values():
                precio_formateado = format_price(item['precio'] * item['cantidad'], tenant_info['currency'])
                resumen_productos += f"‚Ä¢ {item['cantidad']} x {item['nombre']} = {precio_formateado}\n"
            
            total_formateado = format_price(total, tenant_info['currency'])
            respuesta = f"""üéâ **¬°Pedido confirmado!** #{pedido.id}

üõí **Tu compra:**
{resumen_productos}
üí∞ **Total: {total_formateado}**

üí≥ **Para completar tu pedido:**
üëâ Haz clic aqu√≠ para pagar: {url_pago}

‚è∞ **Despu√©s del pago:**
Escribe *"pagado"* y verificaremos tu pago autom√°ticamente."""
            
            guardar_sesion(db, sesion, "ORDER_SCHEDULING", {"pedido_id": pedido.id})
            return respuesta
            
        elif any(word in mensaje_lower for word in ["no", "cancelar", "cancel"]):
            guardar_sesion(db, sesion, "INITIAL", {})
            return "‚ùå **Pedido cancelado**\n\n¬øEn qu√© m√°s puedo ayudarte?"
        
        # Si escribe algo diferente durante la confirmaci√≥n
        else:
            return f"‚ùì No entend√≠ tu respuesta.\n\n‚ö° **Responde claramente:**\n‚Ä¢ **S√ç** - para confirmar el pedido\n‚Ä¢ **NO** - para cancelar\n\nüîÑ ¬øConfirmas tu pedido?"
    
    # Verificar pedido pendiente de pago
    pedido_pendiente = db.query(FlowPedido).filter_by(
        telefono=telefono,
        estado="pendiente_pago"
    ).first()
    
    # Si hay pedido pendiente y no est√° cancelando
    if pedido_pendiente and "cancelar" not in mensaje_lower:
        if mensaje_lower in ["pagado", "ya pagu√©", "pague", "pago"]:
            # Verificar estado en BD (actualizado por callback de Flow)
            pedido_actual = db.query(FlowPedido).filter_by(id=pedido_pendiente.id).first()
            if pedido_actual and pedido_actual.estado == "pagado":
                guardar_sesion(db, sesion, "INITIAL", {})
                return f"‚úÖ ¬°Pago confirmado! Tu pedido #{pedido_actual.id} ha sido procesado.\nGracias por tu compra. üôå"
            else:
                return f"‚ö†Ô∏è A√∫n no hemos recibido la confirmaci√≥n del pago para tu pedido #{pedido_pendiente.id}.\n\nSi ya pagaste, el proceso puede tomar unos minutos."
        
        return f"Tu pedido #{pedido_pendiente.id} est√° pendiente de pago. Si ya pagaste escribe *pagado*, o escribe *cancelar pedido* para anularlo."
    
    # Cancelar pedido
    if "cancelar pedido" in mensaje_lower or "cancelar" in mensaje_lower:
        if pedido_pendiente:
            pedido_pendiente.estado = "cancelado"
            db.commit()
            guardar_sesion(db, sesion, "INITIAL", {})
            productos, tenant_id, tenant_info = obtener_productos_cliente_real(db, telefono, tenant_id)
            return f"‚ùå Tu pedido #{pedido_pendiente.id} ha sido *cancelado*.\n" + menu_principal(tenant_info, productos)
        else:
            productos, tenant_id, tenant_info = obtener_productos_cliente_real(db, telefono, tenant_id)
            return "No tienes pedidos pendientes para cancelar.\n" + menu_principal(tenant_info, productos)
    
    # ========================================
    # ELIMINADA DETECCI√ìN HARDCODEADA - AHORA TODO ES DIN√ÅMICO CON GPT
    # El sistema GPT completamente din√°mico maneja todas las intenciones
    # ========================================

    # ========================================
    # PRIORIDAD 2: SISTEMA DE IA MEJORADO CON CONTEXTO  
    # ========================================
    if AI_IMPROVEMENTS_AVAILABLE and OPENAI_AVAILABLE:
        try:
            print(f"ü§ñ Iniciando sistema IA mejorado para: '{mensaje}'")
            
            # Obtener productos para el contexto
            productos, tenant_id, tenant_info = obtener_productos_cliente_real(db, telefono, tenant_id)
            
            if productos:
                # Procesar mensaje con IA mejorada
                respuesta_ia, metadata_ia = process_message_with_ai_improvements(
                    db, telefono, tenant_id, mensaje, productos, tenant_info
                )
                
                print(f"‚úÖ IA Mejorada respondi√≥ (confianza: {metadata_ia.get('intent_confidence', 0):.2f}, tiempo: {metadata_ia.get('response_time_ms', 0)}ms)")
                
                # Si la confianza es alta, usar la respuesta de IA (umbral reducido para testing)
                if metadata_ia.get('intent_confidence', 0) > 0.3:
                    return respuesta_ia
                
                # Si la confianza es media, continuar con smart flows como backup
                print(f"‚ö†Ô∏è Confianza media, continuando con smart flows...")
                
        except Exception as e:
            print(f"‚ùå Error en sistema IA mejorado: {e}")
            # Continuar con smart flows como fallback
    
    # ========================================
    # PRIORIDAD 3: SISTEMA DE FLUJOS INTELIGENTES (FALLBACK)
    # ========================================
    # SMART FLOWS DESHABILITADO - USAR MOTOR GPT REASONING
    if False and SMART_FLOWS_AVAILABLE and OPENAI_AVAILABLE:
        try:
            print(f"üß† Iniciando detecci√≥n inteligente para: '{mensaje}'")
            
            # MANEJO DIN√ÅMICO DE CONTEXTO DE CONVERSACI√ìN USANDO GPT
            # GPT decide si el mensaje es una continuaci√≥n de conversaci√≥n bas√°ndose en el contexto
            contexto_conversacion = _manejar_contexto_dinamico_con_gpt(
                db, sesion, mensaje, datos_sesion, telefono, tenant_id
            )
            
            if contexto_conversacion:
                print(f"üîÑ GPT detect√≥ continuaci√≥n de conversaci√≥n: {contexto_conversacion['accion']}")
                return contexto_conversacion['respuesta']
            
            # Obtener productos para el contexto
            productos, tenant_id, tenant_info = obtener_productos_cliente_real(db, telefono, tenant_id)
            
            if productos:
                # GPT detecta la intenci√≥n espec√≠fica
                deteccion = detectar_intencion_con_gpt(mensaje, productos, tenant_info)
                print(f"üéØ GPT detect√≥: {deteccion}")
                
                # Ejecutar flujo espec√≠fico seg√∫n detecci√≥n
                if deteccion["intencion"] in ["saludo", "consulta_producto", "consulta_categoria", "consulta_catalogo", "intencion_compra"]:
                    print(f"‚úÖ Ejecutando flujo espec√≠fico para: {deteccion['intencion']}")
                    
                    print(f"üöÄ Ejecutando flujo inteligente para: {deteccion['intencion']}")
                    respuesta_inteligente = ejecutar_flujo_inteligente(deteccion, productos, tenant_info)
                    print(f"‚úÖ Flujo inteligente retorn√≥: {respuesta_inteligente[:100]}...")
                    print(f"üìù Respuesta generada: {len(respuesta_inteligente)} caracteres")
                    
                    # Actualizar sesi√≥n seg√∫n el tipo de consulta
                    if deteccion["intencion"] == "consulta_categoria":
                        # Guardar la categor√≠a consultada para mantener contexto
                        categoria_consultada = deteccion.get("categoria")
                        if categoria_consultada:
                            guardar_sesion(db, sesion, "BROWSING", {"ultima_categoria": categoria_consultada})
                        else:
                            guardar_sesion(db, sesion, "BROWSING", {})
                    elif deteccion["intencion"] == "consulta_catalogo":
                        guardar_sesion(db, sesion, "BROWSING", {})
                    elif deteccion["intencion"] == "intencion_compra":
                        # No actualizar sesi√≥n aqu√≠, se maneja en la l√≥gica de compras m√°s abajo
                        pass
                    
                    print("üéâ Flujo inteligente completado exitosamente")
                    return respuesta_inteligente
                    
        except Exception as e:
            print(f"‚ùå Error en flujos inteligentes: {e}")
            import traceback
            traceback.print_exc()
    
    # SOLO SISTEMA GPT - Sin condiciones hardcodeadas
    # El sistema GPT debe manejar todos los casos: saludos, consultas de productos, etc.
    
    # Si llegamos aqu√≠, el sistema GPT no manej√≥ el mensaje, usar fallback GPT
    
    # ========================================
    # MOTOR DE RAZONAMIENTO GPT 100% DIN√ÅMICO - SIN CONDICIONALES
    # ========================================
    
    print(f"üß† Activando motor de razonamiento GPT puro para tenant: {tenant_id}")
    
    try:
        # Importar motor de razonamiento GPT
        from services.gpt_reasoning_engine import process_message_with_pure_gpt_reasoning
        
        # Obtener productos y informaci√≥n del tenant
        productos, tenant_id, tenant_info = obtener_productos_cliente_real(db, telefono, tenant_id)
        
        # Procesar con razonamiento GPT puro - SIN CONDICIONALES
        respuesta_gpt = process_message_with_pure_gpt_reasoning(
            db=db,
            telefono=telefono,
            mensaje=mensaje,
            tenant_id=tenant_id,
            productos=productos
        )
        
        print(f"‚úÖ Motor de razonamiento GPT respondi√≥ para {tenant_id}")
        return respuesta_gpt
        
    except Exception as e:
        print(f"‚ùå Error en motor de razonamiento GPT para {tenant_id}: {e}")
        import traceback
        traceback.print_exc()
        
        # Fallback seguro - m√≠nimo procesamiento
        productos, tenant_id, tenant_info = obtener_productos_cliente_real(db, telefono, tenant_id)
        
        try:
            from services.tenant_config_manager import get_cached_tenant_config
            tenant_config = get_cached_tenant_config(db, tenant_id)
            emoji = " ü§ñ" if tenant_config.use_emojis else ""
            return f"¬°Hola! Soy el asistente de {tenant_config.business_name}{emoji}. ¬øEn qu√© puedo ayudarte?"
        except:
            return f"¬°Hola! Soy el asistente de {tenant_info.get('name', 'nuestra tienda')}. ¬øEn qu√© puedo ayudarte?"

def _manejar_contexto_dinamico_con_gpt(db: Session, sesion, mensaje: str, datos_sesion: dict, telefono: str, tenant_id: str):
    """
    Maneja el contexto de conversaci√≥n de manera 100% din√°mica usando GPT
    Sin l√≥gica hardcodeada - GPT decide si el mensaje es una continuaci√≥n
    """
    if not OPENAI_AVAILABLE:
        return None
    
    try:
        import openai
        client = openai.OpenAI()
        
        # Obtener informaci√≥n del contexto actual
        estado_sesion = sesion.estado
        contexto_anterior = datos_sesion.get('ultima_categoria', '')
        ultima_respuesta = datos_sesion.get('ultima_respuesta', '')
        
        # Solo procesar si hay contexto previo
        if not contexto_anterior and estado_sesion != "BROWSING":
            return None
        
        # PROMPT DIN√ÅMICO PARA AN√ÅLISIS DE CONTEXTO
        contexto_prompt = f"""Analiza si este mensaje contin√∫a una conversaci√≥n previa.

CONVERSACI√ìN PREVIA:
- Estado: {estado_sesion}
- Categor√≠a anterior: {contexto_anterior or 'ninguna'}
- √öltima respuesta: {ultima_respuesta[:100] if ultima_respuesta else 'ninguna'}

MENSAJE ACTUAL: "{mensaje}"

TAREA: ¬øEs este mensaje una continuaci√≥n de la conversaci√≥n anterior?

AN√ÅLISIS:
- Si el usuario responde afirmativamente ‚Üí puede ser continuaci√≥n
- Si cambia de tema completamente ‚Üí nueva conversaci√≥n
- Si hace seguimiento al tema anterior ‚Üí continuaci√≥n

RESPONDE UNA de estas opciones:
- "continuar_categoria" - continuar con la categor√≠a que se consult√≥ antes
- "continuar_catalogo" - mostrar cat√°logo general
- "nueva_conversacion" - iniciar nueva conversaci√≥n

RESPUESTA:"""

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": contexto_prompt}],
            temperature=0.1,
            max_tokens=20
        )
        
        decision_gpt = response.choices[0].message.content.strip().lower()
        print(f"üß† GPT decisi√≥n de contexto: '{decision_gpt}'")
        
        # Procesar decisi√≥n de GPT
        if decision_gpt == "continuar_categoria" and contexto_anterior:
            print(f"üîÑ GPT: Continuando con categor√≠a '{contexto_anterior}'")
            
            # Obtener productos y ejecutar consulta de categor√≠a
            productos, _, tenant_info = obtener_productos_cliente_real(db, telefono, tenant_id)
            from services.smart_flows import ejecutar_consulta_categoria
            respuesta = ejecutar_consulta_categoria(contexto_anterior, productos, tenant_info)
            
            # Mantener contexto
            guardar_sesion(db, sesion, "BROWSING", {
                "ultima_categoria": contexto_anterior,
                "ultima_respuesta": respuesta[:200]
            })
            
            return {
                "accion": "continuar_categoria",
                "respuesta": respuesta
            }
            
        elif decision_gpt == "continuar_catalogo":
            print(f"üîÑ GPT: Continuando con cat√°logo completo")
            
            # Mostrar cat√°logo completo
            productos, _, tenant_info = obtener_productos_cliente_real(db, telefono, tenant_id)
            from services.smart_flows import ejecutar_consulta_catalogo
            respuesta = ejecutar_consulta_catalogo(productos, tenant_info)
            
            # Actualizar contexto
            guardar_sesion(db, sesion, "BROWSING", {
                "ultima_respuesta": respuesta[:200]
            })
            
            return {
                "accion": "continuar_catalogo", 
                "respuesta": respuesta
            }
        
        else:
            print(f"üîÑ GPT: Nueva conversaci√≥n - no hay continuaci√≥n")
            return None
            
    except Exception as e:
        print(f"‚ùå Error en manejo din√°mico de contexto: {e}")
        return None